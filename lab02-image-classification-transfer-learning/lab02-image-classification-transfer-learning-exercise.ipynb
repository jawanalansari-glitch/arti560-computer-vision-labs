{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430e7f7b",
   "metadata": {},
   "source": [
    "##### ARTI 560 - Computer Vision  \n",
    "## Image Classification using Transfer Learning - Exercise \n",
    "\n",
    "### Objective\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "1. Select another pretrained model (e.g., VGG16, MobileNetV2, or EfficientNet) and fine-tune it for CIFAR-10 classification.  \n",
    "You'll find the pretrained models in [Tensorflow Keras Applications Module](https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n",
    "\n",
    "2. Before training, inspect the architecture using model.summary() and observe:\n",
    "- Network depth\n",
    "- Number of parameters\n",
    "- Trainable vs Frozen layers\n",
    "\n",
    "3. Then compare its performance with ResNet and the custom CNN.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "- Which model achieved the highest accuracy?\n",
    "- Which model trained faster?\n",
    "- How might the architecture explain the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69eb299c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cifar10_mobilenetv2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cifar10_mobilenetv2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resizing (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Resizing</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ augmentation (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resizing (\u001b[38;5;33mResizing\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m12,810\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,270,794</span> (8.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,270,794\u001b[0m (8.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> (50.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,810\u001b[0m (50.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backbone depth (layers): 154\n",
      "Backbone params: 2257984\n",
      "Trainable layers (backbone): 0 / 154\n",
      "Epoch 1/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 102ms/step - accuracy: 0.5907 - loss: 1.1673 - val_accuracy: 0.8200 - val_loss: 0.5271 - learning_rate: 0.0010\n",
      "Epoch 2/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 102ms/step - accuracy: 0.7402 - loss: 0.7454 - val_accuracy: 0.8190 - val_loss: 0.5306 - learning_rate: 0.0010\n",
      "Epoch 3/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 104ms/step - accuracy: 0.7616 - loss: 0.6815 - val_accuracy: 0.8370 - val_loss: 0.4741 - learning_rate: 5.0000e-04\n",
      "\n",
      "MobileNetV2 (frozen) test accuracy: 0.8300999999046326\n",
      "MobileNetV2 (frozen) test loss    : 0.4937940835952759\n",
      "MobileNetV2 (frozen) training time: 226.50286436080933 sec\n",
      "\n",
      "After unfreezing last 30 layers:\n",
      "Trainable layers (backbone): 30 / 154\n",
      "Epoch 1/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 138ms/step - accuracy: 0.6730 - loss: 0.9517 - val_accuracy: 0.8280 - val_loss: 0.5007 - learning_rate: 1.0000e-05\n",
      "Epoch 2/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 136ms/step - accuracy: 0.7720 - loss: 0.6496 - val_accuracy: 0.8340 - val_loss: 0.4670 - learning_rate: 1.0000e-05\n",
      "Epoch 3/3\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 136ms/step - accuracy: 0.7940 - loss: 0.5961 - val_accuracy: 0.8518 - val_loss: 0.4148 - learning_rate: 1.0000e-05\n",
      "\n",
      "MobileNetV2 (fine-tuned) test accuracy: 0.8529999852180481\n",
      "MobileNetV2 (fine-tuned) test loss    : 0.42899173498153687\n",
      "MobileNetV2 (fine-tuned) training time: 299.446670293808 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load CIFAR-10\n",
    "# -----------------------------\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "y_train = y_train.squeeze().astype(\"int64\")\n",
    "y_test  = y_test.squeeze().astype(\"int64\")\n",
    "\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test  = x_test.astype(\"float32\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Data augmentation (reuse yours if you want)\n",
    "# -----------------------------\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.1),\n",
    "], name=\"augmentation\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build MobileNetV2 backbone (pretrained)\n",
    "# -----------------------------\n",
    "mobilenet_base = MobileNetV2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "mobilenet_base.trainable = False\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Full model (preprocess inside model)\n",
    "# -----------------------------\n",
    "mobilenet_model = keras.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),\n",
    "    data_augmentation,\n",
    "    layers.Resizing(224, 224, interpolation=\"bilinear\"),\n",
    "    layers.Lambda(mobilenet_preprocess),   # IMPORTANT: MobileNetV2 preprocessing\n",
    "    mobilenet_base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(10)                       # logits\n",
    "], name=\"cifar10_mobilenetv2\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Inspect architecture\n",
    "# -----------------------------\n",
    "mobilenet_model.summary()\n",
    "\n",
    "print(\"\\nBackbone depth (layers):\", len(mobilenet_base.layers))\n",
    "print(\"Backbone params:\", mobilenet_base.count_params())\n",
    "print(\"Trainable layers (backbone):\", sum(l.trainable for l in mobilenet_base.layers), \"/\", len(mobilenet_base.layers))\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Compile + Train (frozen)\n",
    "# -----------------------------\n",
    "mobilenet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1),\n",
    "]\n",
    "\n",
    "t0 = time.time()\n",
    "history_mn_frozen = mobilenet_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "frozen_time = time.time() - t0\n",
    "\n",
    "test_loss_mn, test_acc_mn = mobilenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"\\nMobileNetV2 (frozen) test accuracy:\", test_acc_mn)\n",
    "print(\"MobileNetV2 (frozen) test loss    :\", test_loss_mn)\n",
    "print(\"MobileNetV2 (frozen) training time:\", frozen_time, \"sec\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Fine-tune last layers\n",
    "# -----------------------------\n",
    "mobilenet_base.trainable = True\n",
    "\n",
    "# Freeze most layers, unfreeze last N layers (tune this value)\n",
    "N = 30\n",
    "for layer in mobilenet_base.layers[:-N]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"\\nAfter unfreezing last\", N, \"layers:\")\n",
    "print(\"Trainable layers (backbone):\", sum(l.trainable for l in mobilenet_base.layers), \"/\", len(mobilenet_base.layers))\n",
    "\n",
    "mobilenet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "history_mn_ft = mobilenet_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "ft_time = time.time() - t1\n",
    "\n",
    "test_loss_mn_ft, test_acc_mn_ft = mobilenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"\\nMobileNetV2 (fine-tuned) test accuracy:\", test_acc_mn_ft)\n",
    "print(\"MobileNetV2 (fine-tuned) test loss    :\", test_loss_mn_ft)\n",
    "print(\"MobileNetV2 (fine-tuned) training time:\", ft_time, \"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7b6c4",
   "metadata": {},
   "source": [
    "- Custom CNN\n",
    "\n",
    "Test Accuracy: 70.28%\n",
    "\n",
    "Training Time: ~110 sec (10 epochs)\n",
    "\n",
    "Architecture: 2 Conv layers + Dense\n",
    "\n",
    "- ResNet50V2\n",
    "Frozen:\n",
    "\n",
    "Test Accuracy: 87.42%\n",
    "\n",
    "Test Loss: 0.3587\n",
    "\n",
    "Fine-Tuned:\n",
    "\n",
    "Test Accuracy: 91.62%\n",
    "\n",
    "Test Loss: 0.2423\n",
    "\n",
    "Parameters: 23.5M\n",
    "\n",
    "Backbone depth: 190 layers\n",
    "\n",
    "- MobileNetV2\n",
    "Frozen:\n",
    "\n",
    "Test Accuracy: 83.01%\n",
    "\n",
    "Training Time: 226 sec\n",
    "\n",
    "Fine-Tuned:\n",
    "\n",
    "Test Accuracy: 85.30%\n",
    "\n",
    "Training Time: 299 sec\n",
    "\n",
    "Parameters: 2.27M\n",
    "\n",
    "Depth: 154 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26d77e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee5bcaf",
   "metadata": {},
   "source": [
    "- Which model achieved the highest accuracy?\n",
    "ResNet50V2 (fine-tuned) with 91.62%.\n",
    "\n",
    "- Which model trained faster?\n",
    "Custom CNN trained the fastest overall. Among pretrained models, MobileNetV2 was faster than ResNet.\n",
    "\n",
    "- How might the architecture explain the differences?\n",
    "Custom CNN has limited depth and was trained from scratch, so it lacks the rich feature representations learned from large-scale datasets.\n",
    "\n",
    "MobileNetV2 uses depthwise separable convolutions and inverted residual blocks. This significantly reduces computation while maintaining strong feature extraction capability, leading to good accuracy with efficient training.\n",
    "\n",
    "ResNet50V2 uses residual connections that allow very deep networks to train effectively without vanishing gradients. Its greater depth and higher parameter count enable superior feature learning, resulting in the highest accuracy, but at the cost of longer training time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
